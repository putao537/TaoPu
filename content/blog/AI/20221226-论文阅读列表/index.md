---
title: '论文阅读列表'
subtitle: 
summary: 记录看过人工智能相关的论文.
authors:
tags:
- Paper
- Deep Learning
categories:
- AI
date: "2022-12-26T00:00:00Z"
lastmod: "2022-12-26T00:00:00Z"
featured: false
draft: false
---

## Visual Reasoning

### Multi-label Image Recognition
- Texts as Images in Prompt Tuning for Multi-Label Image Recognition, arxiv 2022, [[PDF]](https://arxiv.org/abs/2211.12739)  
- DualCoOp: Fast Adaptation to Multi-Label Recognition with Limited Annotations, arxiv 2022, [[PDF]](https://arxiv.org/abs/2206.09541)  
- General Multi-label Image Classification with Transformers, CVPR 2021, [[PDF]](https://arxiv.org/abs/2011.14027)
- Learning Semantic-Specific Graph Representation for Multi-Label Image Recognition, ICCV 2019, [[PDF]](https://arxiv.org/abs/1908.07325)

### Visual Relationship Detection
- Dynamic Scene Graph Generation via Anticipatory Pre-training, CVPR 2022, [[PDF]](https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Dynamic_Scene_Graph_Generation_via_Anticipatory_Pre-Training_CVPR_2022_paper.pdf)   
- Spatial-Temporal Transformer for Dynamic Scene Graph Generation, ICCV 2021, [[PDF]](https://arxiv.org/abs/2107.12309)

### Scene Parsing
- Robust Scene Parsing by Mining Supportive Knowledge From Dataset, T-NNLS 2021, [[PDF]](https://ieeexplore.ieee.org/document/9537741)

## Embodied AI

### General Embodied Agent
- MINEDOJO: Building Open-Ended Embodied Agents with Internet-Scale Knowledge, NeurIPS 2022, [[PDF]](https://arxiv.org/abs/2206.08853)  


## Pre-training & Fine-tuning

### Computer Vision
- A Unified Sequence Interface for Vision Tasks, arxiv 2022, [[PDF]](https://arxiv.org/abs/2206.07669)  
- Learning to Prompt for Vision-Language Models, IJCV 2022, [[PDF]](https://arxiv.org/abs/2109.01134)  
- Learning Transferable Visual Models From Natural Language Supervision, ICML 2021, [[PDF]](https://arxiv.org/abs/2103.00020)

### Natural Language Processing
- Language Models are Few-Shot Learners, NeurIPS 2020, [[PDF]](https://arxiv.org/abs/2005.14165)
- Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context, ACL 2019, [[PDF]](https://arxiv.org/abs/1901.02860)
- XLNet: Generalized Autoregressive Pretraining for Language Understanding, arxiv 2019, [[PDF]](https://arxiv.org/abs/1906.08237)
- BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, NAACL 2019, [[PDF]](https://arxiv.org/abs/1810.04805)
- Language Models are Unsupervised Multitask Learners, OpenAI Blog 2019, [[PDF]](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) 
- Improving Language Understanding by Generative Pre-Training, OpenAI Blog 2018, [[PDF]](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)
- Deep contextualized word representations, NAACL 2018, [[PDF]](https://arxiv.org/abs/1802.05365)

### Robotic
- RT-1: Robotics Transformer for Real-World Control at Scale, arxiv 2022, [[PDF]](https://robotics-transformer.github.io/assets/rt1.pdf)
